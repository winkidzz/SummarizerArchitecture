"""
Prometheus Metrics Collection for RAG System

This module provides comprehensive metrics collection for monitoring
the RAG pipeline performance, quality, and operational health.
"""

from prometheus_client import Counter, Histogram, Gauge, Summary
from typing import Optional
import time

# ============================================================================
# Query Metrics
# ============================================================================

query_total = Counter(
    'rag_queries_total',
    'Total number of queries processed',
    ['embedder_type', 'status', 'cache_status']
)

query_duration = Histogram(
    'rag_query_duration_seconds',
    'End-to-end query processing duration',
    ['embedder_type'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

query_tokens = Histogram(
    'rag_query_tokens',
    'Token usage per query',
    ['stage', 'type']  # stage: input/output, type: query/generation
)

# ============================================================================
# Retrieval Metrics
# ============================================================================

retrieval_docs = Histogram(
    'rag_retrieval_docs',
    'Number of documents retrieved',
    ['retriever_type', 'stage'],  # stage: initial/reranked
    buckets=[1, 5, 10, 20, 50, 100]
)

retrieval_duration = Histogram(
    'rag_retrieval_duration_seconds',
    'Retrieval operation duration',
    ['retriever_type', 'stage'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]
)

retrieval_scores = Histogram(
    'rag_retrieval_scores',
    'Retrieval similarity scores',
    ['retriever_type'],
    buckets=[0.0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0]
)

retrieval_quality = Histogram(
    'rag_retrieval_quality_score',
    'Retrieval quality score (composite metric)',
    ['embedder_type'],
    buckets=[0.0, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

# ============================================================================
# Generation Metrics
# ============================================================================

generation_duration = Histogram(
    'rag_generation_duration_seconds',
    'LLM generation duration',
    ['model_type'],
    buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

generation_tokens = Histogram(
    'rag_generation_tokens',
    'Tokens generated by LLM',
    ['model_type'],
    buckets=[100, 500, 1000, 2000, 5000, 10000]
)

generation_errors = Counter(
    'rag_generation_errors_total',
    'Generation errors',
    ['error_type', 'model_type']
)

answer_length = Histogram(
    'rag_answer_length',
    'Answer length in characters',
    buckets=[100, 500, 1000, 2000, 5000, 10000]
)

citation_count = Histogram(
    'rag_citation_count',
    'Number of citations per answer',
    buckets=[0, 1, 2, 3, 5, 10, 20]
)

# ============================================================================
# Embedding Metrics
# ============================================================================

embedding_duration = Histogram(
    'rag_embedding_duration_seconds',
    'Embedding generation duration',
    ['embedder_type'],
    buckets=[0.001, 0.01, 0.05, 0.1, 0.5, 1.0]
)

embedding_requests = Counter(
    'rag_embedding_requests_total',
    'Total embedding requests',
    ['embedder_type', 'status']
)

embedder_usage = Counter(
    'rag_embedder_usage_total',
    'Embedder type usage',
    ['embedder_type']
)

# ============================================================================
# Cache Metrics
# ============================================================================

cache_hits = Counter(
    'rag_cache_hits_total',
    'Cache hits',
    ['cache_type']
)

cache_misses = Counter(
    'rag_cache_misses_total',
    'Cache misses',
    ['cache_type']
)

cache_size = Gauge(
    'rag_cache_size',
    'Current cache size in entries',
    ['cache_type']
)

cache_operations = Counter(
    'rag_cache_operations_total',
    'Cache operations',
    ['operation', 'cache_type']  # operation: get/set/evict
)

# ============================================================================
# System Metrics
# ============================================================================

vector_store_size = Gauge(
    'rag_vector_store_size',
    'Total documents in vector store'
)

vector_store_operations = Counter(
    'rag_vector_store_operations_total',
    'Vector store operations',
    ['operation', 'status']  # operation: insert/query/delete
)

elasticsearch_operations = Counter(
    'rag_elasticsearch_operations_total',
    'Elasticsearch operations',
    ['operation', 'status']
)

redis_operations = Counter(
    'rag_redis_operations_total',
    'Redis operations',
    ['operation', 'status']
)

# ============================================================================
# Quality Metrics
# ============================================================================

user_feedback = Counter(
    'rag_user_feedback_total',
    'User feedback ratings',
    ['rating']  # rating: positive/negative/neutral
)

query_complexity = Histogram(
    'rag_query_complexity',
    'Query complexity score',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

# ============================================================================
# Classical IR Metrics
# ============================================================================

retrieval_precision_at_k = Histogram(
    'rag_retrieval_precision_at_k',
    'Precision at K for retrieved documents',
    ['k_value'],
    buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

retrieval_recall_at_k = Histogram(
    'rag_retrieval_recall_at_k',
    'Recall at K for retrieved documents',
    ['k_value'],
    buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

retrieval_hit_rate_at_k = Histogram(
    'rag_retrieval_hit_rate_at_k',
    'Hit rate at K (whether any relevant doc in top-K)',
    ['k_value'],
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

retrieval_mrr = Histogram(
    'rag_retrieval_mrr',
    'Mean Reciprocal Rank of first relevant document',
    buckets=[0.0, 0.1, 0.2, 0.33, 0.5, 0.67, 0.8, 0.9, 1.0]
)

retrieval_map = Histogram(
    'rag_retrieval_map',
    'Mean Average Precision across all relevant documents',
    buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

retrieval_ndcg_at_k = Histogram(
    'rag_retrieval_ndcg_at_k',
    'Normalized Discounted Cumulative Gain at K',
    ['k_value'],
    buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

# ============================================================================
# Context Quality Metrics
# ============================================================================

context_precision = Histogram(
    'rag_context_precision',
    'Fraction of retrieved context that is relevant to query',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

context_recall = Histogram(
    'rag_context_recall',
    'Whether retrieved context covers all facts needed to answer query',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

context_relevancy = Histogram(
    'rag_context_relevancy',
    'Proportion of retrieved context that is relevant (irrespective of rank)',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

context_utilization = Histogram(
    'rag_context_utilization',
    'Ratio of retrieved context actually used in generated answer',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

# ============================================================================
# Answer Quality Metrics
# ============================================================================

answer_faithfulness_score = Histogram(
    'rag_answer_faithfulness_score',
    'Faithfulness score - whether answer claims are supported by context',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

hallucination_detected = Counter(
    'rag_hallucination_detected',
    'Count of detected hallucinations (unsupported claims)',
    ['severity']  # minor/moderate/severe
)

answer_relevancy_score = Histogram(
    'rag_answer_relevancy_score',
    'Embedding similarity between query and answer',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 1.0]
)

answer_completeness_score = Histogram(
    'rag_answer_completeness_score',
    'Whether answer includes all relevant information',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

citation_grounding_score = Histogram(
    'rag_citation_grounding_score',
    'Whether cited sources actually support the claims made',
    buckets=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)

# ============================================================================
# Web Search Metrics (Phase 1)
# ============================================================================

web_search_queries = Counter(
    'rag_web_search_queries_total',
    'Total web search queries',
    ['mode', 'status']  # mode: parallel/on_low_confidence, status: success/error
)

web_search_results = Histogram(
    'rag_web_search_results',
    'Number of web search results returned',
    buckets=[0, 1, 3, 5, 10, 20]
)

web_search_duration = Histogram(
    'rag_web_search_duration_seconds',
    'Web search query duration',
    ['provider'],  # provider: duckduckgo, tavily, etc.
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

web_search_trust_scores = Histogram(
    'rag_web_search_trust_scores',
    'Web search result trust scores',
    buckets=[0.0, 0.3, 0.5, 0.7, 0.9, 1.0]
)

web_source_ratio = Histogram(
    'rag_web_source_ratio',
    'Ratio of web results in final ranking (0.0-1.0)',
    buckets=[0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]
)

# ============================================================================
# Helper Functions
# ============================================================================

class MetricsCollector:
    """Helper class for collecting metrics with context."""

    # Web search metrics as class attributes (Phase 1)
    web_search_queries = web_search_queries
    web_search_results = web_search_results
    web_search_duration = web_search_duration
    web_search_trust_scores = web_search_trust_scores
    web_source_ratio = web_source_ratio

    @staticmethod
    def record_query(
        embedder_type: str,
        status: str,
        cache_hit: bool,
        duration: float,
        retrieved_docs: int,
        answer_length: int,
        citation_count: int
    ):
        """Record query metrics."""
        cache_status = "hit" if cache_hit else "miss"
        query_total.labels(
            embedder_type=embedder_type,
            status=status,
            cache_status=cache_status
        ).inc()
        
        query_duration.labels(embedder_type=embedder_type).observe(duration)
        retrieval_docs.labels(
            retriever_type="hybrid",
            stage="final"
        ).observe(retrieved_docs)
        # Access module-level metric objects via globals() to avoid parameter shadowing
        globals()['answer_length'].observe(answer_length)
        globals()['citation_count'].observe(citation_count)
    
    @staticmethod
    def record_retrieval(
        retriever_type: str,
        stage: str,
        duration: float,
        doc_count: int,
        avg_score: Optional[float] = None
    ):
        """Record retrieval metrics."""
        retrieval_duration.labels(
            retriever_type=retriever_type,
            stage=stage
        ).observe(duration)
        retrieval_docs.labels(
            retriever_type=retriever_type,
            stage=stage
        ).observe(doc_count)
        
        if avg_score is not None:
            retrieval_scores.labels(retriever_type=retriever_type).observe(avg_score)
    
    @staticmethod
    def record_generation(
        model_type: str,
        duration: float,
        tokens: int
    ):
        """Record generation metrics."""
        generation_duration.labels(model_type=model_type).observe(duration)
        generation_tokens.labels(model_type=model_type).observe(tokens)
    
    @staticmethod
    def record_embedding(
        embedder_type: str,
        duration: float,
        status: str = "success"
    ):
        """Record embedding metrics."""
        embedding_duration.labels(embedder_type=embedder_type).observe(duration)
        embedding_requests.labels(
            embedder_type=embedder_type,
            status=status
        ).inc()
        embedder_usage.labels(embedder_type=embedder_type).inc()
    
    @staticmethod
    def record_cache(
        cache_type: str,
        operation: str,
        hit: Optional[bool] = None
    ):
        """Record cache metrics."""
        cache_operations.labels(
            operation=operation,
            cache_type=cache_type
        ).inc()
        
        if hit is not None:
            if hit:
                cache_hits.labels(cache_type=cache_type).inc()
            else:
                cache_misses.labels(cache_type=cache_type).inc()
    
    @staticmethod
    def update_cache_size(cache_type: str, size: int):
        """Update cache size metric."""
        cache_size.labels(cache_type=cache_type).set(size)
    
    @staticmethod
    def update_vector_store_size(size: int):
        """Update vector store size metric."""
        vector_store_size.set(size)
    
    @staticmethod
    def record_error(error_type: str, model_type: str = "unknown"):
        """Record error metric."""
        generation_errors.labels(
            error_type=error_type,
            model_type=model_type
        ).inc()

    @staticmethod
    def record_ir_metrics(
        precision_at_k: dict,
        recall_at_k: dict,
        hit_rate_at_k: dict,
        mrr: float,
        map_score: float,
        ndcg_at_k: dict
    ):
        """
        Record classical Information Retrieval metrics.

        Args:
            precision_at_k: Dict mapping k values to precision scores
            recall_at_k: Dict mapping k values to recall scores
            hit_rate_at_k: Dict mapping k values to hit rates
            mrr: Mean Reciprocal Rank score
            map_score: Mean Average Precision score
            ndcg_at_k: Dict mapping k values to NDCG scores
        """
        for k, score in precision_at_k.items():
            retrieval_precision_at_k.labels(k_value=str(k)).observe(score)

        for k, score in recall_at_k.items():
            retrieval_recall_at_k.labels(k_value=str(k)).observe(score)

        for k, score in hit_rate_at_k.items():
            retrieval_hit_rate_at_k.labels(k_value=str(k)).observe(score)

        retrieval_mrr.observe(mrr)
        retrieval_map.observe(map_score)

        for k, score in ndcg_at_k.items():
            retrieval_ndcg_at_k.labels(k_value=str(k)).observe(score)

    @staticmethod
    def record_context_quality(
        precision: float,
        recall: float,
        relevancy: float,
        utilization: float
    ):
        """
        Record context quality metrics.

        Args:
            precision: Fraction of retrieved context that is relevant
            recall: Whether context covers all needed facts
            relevancy: Proportion of relevant context
            utilization: Ratio of context used in answer
        """
        context_precision.observe(precision)
        context_recall.observe(recall)
        context_relevancy.observe(relevancy)
        context_utilization.observe(utilization)

    @staticmethod
    def record_answer_quality(
        faithfulness: float,
        relevancy: float,
        completeness: float,
        citation_grounding: float,
        has_hallucination: bool = False,
        hallucination_severity: str = "minor"
    ):
        """
        Record answer quality metrics.

        Args:
            faithfulness: Whether answer is supported by context
            relevancy: Similarity between query and answer
            completeness: Whether answer includes all relevant info
            citation_grounding: Whether citations support claims
            has_hallucination: Whether hallucination detected
            hallucination_severity: Severity level (minor/moderate/severe)
        """
        answer_faithfulness_score.observe(faithfulness)
        answer_relevancy_score.observe(relevancy)
        answer_completeness_score.observe(completeness)
        citation_grounding_score.observe(citation_grounding)

        if has_hallucination:
            hallucination_detected.labels(severity=hallucination_severity).inc()

