Sidebar menu

Write
Notifications

Win Kidzz
Member-only story
I Rebuilt My RAG Pipeline 11 Times ‚Äî Here‚Äôs the Architecture That Finally Scaled
How I went from $8,400/month cloud costs and 18-second queries to $1,240/month and 420ms response times after 11 failed attempts.

CodeOrbit
CodeOrbit

Follow
9 min read
¬∑
Nov 9, 2025
227

6




Press enter or click to view image in full size

Ai generated Image
I watched my monitoring dashboard light up red while our CTO demoed the system to investors. 18-second query latencies. Memory consumption spiking to 64GB. The embarrassment was crushing, but what really hurt was realizing I‚Äôd been solving the wrong problems for 14 months.
This isn‚Äôt a ‚ÄúI built RAG in a weekend‚Äù tutorial. This is the story of 11 failed architectures, $67,000 in wasted cloud costs, and the system that finally handles 250,000 documents with 420ms response times.
If you‚Äôre building RAG at scale, I‚Äôm about to save you at least six months of pain.
Press enter or click to view image in full size

Ai generated Image
The Lie Everyone Believes: ‚ÄúJust Add a Vector Database‚Äù
Every RAG tutorial follows the same script:
Chunk documents ‚Üí Generate embeddings ‚Üí Store in Pinecone ‚Üí Ship it.
Sounds simple, right? That‚Äôs what killed my first three attempts.
Here‚Äôs what actually happens in production:
At 10K documents: Everything‚Äôs perfect. Queries return in 800ms. You feel like a genius. Your team thinks you‚Äôre the next Andrew Ng.
At 50K documents: Things get weird. Memory doubles every week. Some queries timeout for ‚Äúno reason.‚Äù You start Googling ‚ÄúPinecone alternatives‚Äù at 2 AM.
At 250K documents: Complete collapse. Query timeouts hit 89%. Your embedding pipeline runs for 72 hours straight. AWS sends you a fraud alert about your $12K bill.
The problem isn‚Äôt scale. It‚Äôs that every RAG component has hidden bottlenecks that only reveal themselves when you‚Äôre already screwed.
Attempt #1‚Äì4: The Expensive Education ($47K Lesson)
Press enter or click to view image in full size

Ai generated Image
Attempt 1: LangChain + Pinecone + OpenAI Embeddings
The Marketing Promise: ‚ÄúProduction-ready in 2 weeks!‚Äù
The Reality: Collapsed at 15K documents
Monthly Cost: $4,200
What Killed It: Embedding costs. Every tiny document update meant re-embedding everything. Each query cost $0.18.
# What I thought would work
vectorstore = Pinecone.from_documents(
    documents,  # All 250K of them
    OpenAIEmbeddings(),  # Ka-ching! üí∏
    index_name="my-index"
)

# Result:
# - Query time: 3.2 seconds
# - Cost per query: $0.18
# - Monthly bill: *screaming*
Lesson: LangChain is perfect for demos. For production, you need control. Lots of it.
Attempt 2: Open Source Everything (The Pendulum Swing)
The Promise: ‚ÄúCut costs with Sentence Transformers!‚Äù
The Reality: Retrieval accuracy dropped to 43%
What Killed It: Cheap embeddings gave me cheap results. Users couldn‚Äôt find anything. Legal contracts and medical records became indistinguishable blobs.
Lesson: Saving money on embeddings costs you 10x more in user frustration.
Attempt 3: Elasticsearch Hybrid Search (Almost There)
The Promise: ‚ÄúHybrid search solves everything!‚Äù
The Reality: 8.7 second query latency
What Killed It: Sequential processing. BM25 search ‚Üí filter ‚Üí vector search ‚Üí rerank. Each step added 1‚Äì2 seconds. The architecture was sound. The implementation was a disaster.
Lesson: Good ideas executed poorly are just expensive failures.
Attempt 4: Custom Everything (The Over-Engineering Phase)
I built seven reranking models. Three different chunking strategies. Five embedding approaches.
Result: 2% better accuracy. 4x the complexity. Zero chance anyone could maintain it after me.
Lesson: Stop adding complexity until you understand why the simple version failed.
The Breakthrough: What Actually Works
Press enter or click to view image in full size

Ai generated Image
After burning through seven attempts and enough AWS credits to buy a used car, I stopped chasing ‚Äúbest practices‚Äù and started measuring everything.
I built a test harness that simulated 250K documents with real query patterns. Every architectural decision got benchmarked on three metrics: latency, accuracy, cost.
Here‚Äôs the architecture that survived:
Layer 1: Document Processing (The Foundation Everyone Ignores)
Most RAG failures start here. You can‚Äôt retrieve what you didn‚Äôt extract properly.
The Problem: PDFs are chaos. My initial extraction was losing 30% of meaningful content. Tables became gibberish. Headers vanished. Equations turned into ÔøΩÔøΩÔøΩ.
The Solution: Multi-stage extraction with fallbacks.
class RobustPDFExtractor:
    def extract(self, pdf_path: str):
        # Stage 1: Try fast extraction
        try:
            result = self._pypdf_extract(pdf_path)
            if result.confidence > 0.85:
                return result
        except:
            pass  # Failed? Move on.
        
        # Stage 2: Try table-aware extraction
        try:
            result = self._pdfplumber_extract(pdf_path)
            if result.confidence > 0.75:
                return result
        except:
            pass
        
        # Stage 3: Nuclear option - AWS Textract
        return self._textract_extract(pdf_path)  # $$$
Three-stage fallback strategy. Fast path for clean PDFs. Expensive OCR only when needed.
Impact: Extraction quality: 70% ‚Üí 94%. Tables that were gibberish became structured data.
Layer 2: Semantic Chunking (Not Another Fixed-Size Disaster)
Fixed-size chunking is a trap. It splits paragraphs mid-sentence, separates code from comments, and breaks context at random points.
The Problem: My chunks were finding fragments, not answers. ‚ÄúPayment terms are‚Ä¶‚Äù (chunk ends). Users were furious.
The Solution: Respect document structure.
class SemanticChunker:
    def chunk_document(self, text: str, metadata: dict):
        # Detect sections first
        sections = self._detect_sections(text)
        chunks = []
        
        for section in sections:
            if self._is_atomic(section):
                # Keep small sections whole
                chunks.append(section)
            else:
                # Split large sections with overlap
                chunks.extend(
                    self._split_with_overlap(section, size=512, overlap=100)
                )
        
        return chunks
Detect headers. Preserve section boundaries. Add overlap only when needed.
Impact: Retrieval recall jumped from 61% to 87%. Users finally got complete answers.
Layer 3: The Embedding Strategy ($47K Lesson)
This is where I burned the most money.
Failed Approach: ‚ÄúUse the biggest model for maximum quality!‚Äù
I embedded 250K documents with text-embedding-3-large. The bill was $47,000 over six months. The quality improvement? About 3%.
The Solution: Hybrid embedding strategy.
class HybridEmbedder:
    def __init__(self):
        # Fast local model for bulk indexing (FREE)
        self.local_model = SentenceTransformer('all-MiniLM-L12-v2')
        # Premium for queries (low volume)
        self.query_model = "text-embedding-3-small"
    
    def embed_documents(self, texts):
        # Use free local embeddings
        return self.local_model.encode(texts)
    
    def embed_query(self, query):
        # Use OpenAI for queries only
        return openai.embeddings.create(
            model=self.query_model, 
            input=query
        )
The Insight: Embedding 250K documents once vs. embedding 3,400 queries daily. Use expensive models where volume is low.
Impact: Monthly costs: $7,800 ‚Üí $340. Quality: same 94%.
Layer 4: Vector Database (After Testing Six Options)
I tested Pinecone, Weaviate, Qdrant, Milvus, Chroma, and PGVector.
Winner: Qdrant with scalar quantization.
Why Qdrant:
Open source (no vendor lock-in)
Quantization cut memory 4x
On-disk storage for huge indexes
Payload filtering before vector search
# The config that saved my RAM
self.client.create_collection(
    collection_name="docs",
    vectors_config=VectorParams(
        size=384,
        distance=Distance.COSINE,
        on_disk=True  # CRITICAL
    ),
    quantization_config={
        "scalar": {
            "type": "int8",  # 4x smaller
            "quantile": 0.99
        }
    }
)
Impact: Memory usage: 48GB ‚Üí 11GB. Query latency: 2.1s ‚Üí 340ms.
Layer 5: Hybrid Retrieval (The Secret Sauce)
Pure vector search failed 34% of the time on keyword queries. Pure BM25 failed on semantic queries.
The Solution: Three-stage hybrid retrieval.
class HybridRetriever:
    def retrieve(self, query: str, top_k: int = 10):
        # Stage 1: Dense vector search
        dense_results = self.vector_search(query, k=top_k*3)
        
        # Stage 2: Sparse BM25 search
        sparse_results = self.bm25_search(query, k=top_k*3)
        
        # Stage 3: Reciprocal Rank Fusion
        fused = self._rrf_fusion(dense_results, sparse_results)
        
        # Stage 4: Cross-encoder rerank (top 20 only)
        return self._cross_encode_rerank(query, fused[:20])[:top_k]
Get more candidates from both methods. Fuse rankings. Rerank only the best.
Press enter or click to view image in full size

Ai generated Image
Impact:
Recall@10: 62% ‚Üí 91%
MRR: 0.54 ‚Üí 0.83
User satisfaction: 3.2 ‚Üí 4.7 out of 5
Layer 6: Context Window Management (Where Generation Breaks)
Even with perfect retrieval, my LLM was failing. Why? I was shoving 15 documents into context and hoping for the best.
The Problem: Token limits. Irrelevant context. No citation tracking.
The Solution: Intelligent context packing.
class RAGGenerator:
    def generate(self, query: str, docs: List[Dict]):
        # Pack docs intelligently
        context = self._pack_context(docs, max_tokens=8000)
        
        # Build structured prompt
        prompt = self._build_prompt_with_citations(query, context)
        
        # Generate with citations
        response = openai.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1  # Low for accuracy
        )
        
        return {
            "answer": response.choices[0].message.content,
            "sources": self._extract_citations(response)
        }
    
    def _build_prompt_with_citations(self, query, docs):
        context = "\n\n".join([
            f"[Doc {i+1}] Source: {d['source']}\n{d['text']}"
            for i, d in enumerate(docs)
        ])
        
        return f"""Context:
{context}

Question: {query}

Answer using ONLY the context. Cite sources as [Doc X]."""
Impact: Hallucination rate: 31% ‚Üí 4%. Users could verify every claim.
Layer 7: Semantic Caching (The Cost Killer)
This single change saved $18K in three months.
The Insight: 47% of queries were semantically similar. ‚ÄúPayment terms?‚Äù and ‚ÄúTell me about payment conditions‚Äù should hit the same cache.
class SemanticCache:
    def get(self, query_embedding):
        # Check cache for similar queries
        for cached in self.cache_keys:
            similarity = cosine_similarity(query_embedding, cached.embedding)
            if similarity > 0.92:  # Close enough
                return cached.result
        return None
Impact:
Cache hit rate: 47%
Query cost: $0.04 ‚Üí $0.009
Cached response time: 80ms
The Numbers That Keep Me Employed
After 14 months and 11 failed attempts:
Performance:
Average latency: 420ms (was 8.7 seconds)
P95 latency: 890ms
Cache hit rate: 47%
Quality:
Retrieval recall: 91% (was 61%)
Hallucination rate: 4% (was 31%)
User satisfaction: 4.7/5
Cost:
Per query: $0.009 (was $0.18)
Monthly total: $1,240 (was $8,400)
Total saved: $67,000 over 6 months
Scale:
Documents: 250,000+
Daily queries: 3,400+
Uptime: 99.8%
But here‚Äôs the metric that actually matters: lawyers use it instead of Ctrl+F. That‚Äôs the real test.
Five Mistakes That Cost Me $67K
Mistake 1: Trusting ‚ÄúBest Practices‚Äù Without Testing
I spent $12K on Pinecone because ‚Äúeveryone uses it.‚Äù It‚Äôs great ‚Äî if your use case matches theirs. Mine didn‚Äôt.
Lesson: Benchmark everything against YOUR data. E-commerce recommendations ‚â† legal document search.
Mistake 2: Optimizing the Wrong Thing
I spent 3 weeks optimizing embedding speed. My real problem? Retrieval accuracy. Fast wrong answers are still wrong.
Lesson: Fix what users complain about, not what your profiler highlights.
Mistake 3: Treating All Documents the Same
Flat text chunking destroyed context. Legal contracts have sections. Medical records have sequences. Scientific papers have hierarchies.
Lesson: Your chunking needs to understand document structure, not just token counts.
Mistake 4: Over-Engineering Too Early
Attempt #4 had seven reranking models. It was slow, expensive, and 2% better than Attempt #2.
Lesson: Start simple. Add complexity only when metrics prove it helps.
Mistake 5: Not Monitoring Retrieval
I obsessed over LLM outputs while my retriever returned garbage. No amount of prompt engineering fixes bad retrieval.
Lesson: Log every query. Build dashboards. Track ‚ÄúWas this helpful?‚Äù religiously.
What I‚Äôm Testing Next
The system works, but there‚Äôs room for improvement:
Multi-vector retrieval: Generate multiple embeddings per chunk (summary, keywords, questions). Early tests show 12% better recall.
Active learning: Use thumbs up/down to fine-tune retrieval. If users consistently reject a doc, demote it automatically.
Graph enhancement: Connect related chunks. When retrieving a contract clause, auto-include referenced sections.
The goal isn‚Äôt perfection. It‚Äôs building something users trust more than their current workflow.
Press enter or click to view image in full size

Ai generated Image
Start Small, Measure Everything
If you‚Äôre building RAG, here‚Äôs my advice:
Don‚Äôt start with 250K documents. Start with 1,000. Prove your architecture works at small scale before investing in infrastructure.
Measure from day one:
Retrieval quality
End-to-end latency
Cost per query
User satisfaction
Scale gradually:
1K docs ‚Üí validate architecture
10K docs ‚Üí find bottlenecks
50K docs ‚Üí optimize performance
100K+ ‚Üí add caching
Every optimization I made came from a real production problem. The architecture I shared works because I spent 14 months failing first.
Your users don‚Äôt care about your vector database or embedding model. They care whether you give them the right answer faster than their alternative.
Build for that. Everything else is implementation details.
The Real Metric: Do People Actually Use It?
After 11 attempts, the only benchmark that matters is daily active users.
Our legal team runs 3,400 queries daily across 250K documents. Paralegals who spent 4 hours searching files now get answers in seconds. Senior partners who were skeptical are now our biggest advocates.
The system isn‚Äôt perfect. It fails on edge cases. But it‚Äôs reliable enough that people trust it with real work.
That‚Äôs the only metric that counts.
What‚Äôs been your biggest RAG bottleneck? Drop your war stories in the comments. I learn more from production failures than success stories.
This article is based on a production system serving 180+ concurrent users across 4 law firms. All metrics are averaged over 90 days from our monitoring dashboards. Code examples are simplified but functionally accurate.
A Note From the Author
Thanks for making it to the end. The internet is full of RAG tutorials that skip the messy parts.
If this saved you even one failed iteration, it was worth writing.
Hit the claps if it helped, and follow for more stories about production AI systems that actually work in the real world.
Remember: the best architecture is the one that solves your users‚Äô problems. Everything else is overhead.
Rags
Llm
System Architecture
Programming
Coding
227

6



CodeOrbit
Written by CodeOrbit
785 followers
¬∑
284 following
Tech geek unraveling AI wonders, frontend hacks, and blockchain vibes. Let's geek out and innovate! üë®‚Äçüíª #AI #Frontend #Blockchain

Follow
Responses (6)
Win Kidzz
Win Kidzz
What are your thoughts?Ôªø
Cancel
Respond
Krishna Aswatha
Krishna Aswatha
3 days ago

Great article, thank you
I have a question though, in the section "The Embedding Strategy"
You are using two emebeding models which have differenet embeding dimetnions, exmaple Local model (MiniLM-L12-v2) ‚Üí 384 dimensions and OpenAI model ‚Üí 1536‚Ä¶more
15

1 reply
Reply
Andreas Sandberg
Andreas Sandberg
4 days ago

One question, were the embedding models aligned (docs vs queries)? This part confused me a bit as to how you made sure you didn't totally break the retrieval pipeline with two different embedding models.
5

1 reply
Reply
Andreas Sandberg
Andreas Sandberg
4 days ago

Great article!
4

1 reply
Reply
See all responses
More from CodeOrbit
I Built a RAG System for 100,000 Documents‚Ää‚Äî‚ÄäHere‚Äôs the Architecture
CodeOrbit
CodeOrbit
I Built a RAG System for 100,000 Documents‚Ää‚Äî‚ÄäHere‚Äôs the Architecture
My production system crashed at 2 AM because I underestimated vector databases.

Nov 1
833
20


14 Modern HTML + JavaScript Combos That Feel Like Magic in 2025
CodeOrbit
CodeOrbit
14 Modern HTML + JavaScript Combos That Feel Like Magic in 2025
Powerful native features that make frontend development smoother, faster, and a lot more fun.

Oct 31
199
2


15 HTML Features You‚Äôre Not Using (But Should Be)
CodeOrbit
CodeOrbit
15 HTML Features You‚Äôre Not Using (But Should Be)
Discover 15 powerful HTML features you‚Äôre probably ignoring: native accordions, modals, lazy loading, and more. Cut bundle size, boost‚Ä¶

Oct 25
76
3


The Hidden HTML Attribute That Saved Me 500 Lines of JavaScript
CodeOrbit
CodeOrbit
The Hidden HTML Attribute That Saved Me 500 Lines of JavaScript
The browser feature that makes modal libraries obsolete and gives you bulletproof overlays with a single HTML attribute.

Nov 9
83
3


See all from CodeOrbit
Recommended from Medium
Architecture Patterns That Actually Scale In 2025: The Only Three You Need
The Atomic Architect
The Atomic Architect
Architecture Patterns That Actually Scale In 2025: The Only Three You Need
Last month, a company I advised shut down.

Nov 7
1.1K
32


I Deployed My RAG System to Production‚Ää‚Äî‚Ää7 Things That Failed in First Week
CodeOrbit
CodeOrbit
I Deployed My RAG System to Production‚Ää‚Äî‚Ää7 Things That Failed in First Week
Three months building a bulletproof RAG system, six hours in production before disaster struck‚Ää‚Äî‚Äähere are the 7 failures that taught me‚Ä¶

5d ago
61
1


Agentic AI Project: Build a Multi-Agent system with LangGraph and Open AI
Towards AI
In
Towards AI
by
Alpha Iterations
Agentic AI Project: Build a Multi-Agent system with LangGraph and Open AI
This is an end to end project on building a multi-agent insurance support system using Agentic AI [LangGraph and OpenAI API]. [Code‚Ä¶

5d ago
154
2


You Have No Idea How Screwed OpenAI Is
Alberto Romero
Alberto Romero
You Have No Idea How Screwed OpenAI Is
An exhaustive overview of the situation

Nov 5
3.7K
148


Building a Training Architecture for Self-Improving AI Agents
Level Up Coding
In
Level Up Coding
by
Fareed Khan
Building a Training Architecture for Self-Improving AI Agents
RL Algorithms, Policy Modeling, Distributed Training and more.

Nov 4
1.1K
25


How to Monetize Your AI Skills (outside of your 9‚Äì5 job)
The Data Entrepreneurs
In
The Data Entrepreneurs
by
Shaw Talebi
How to Monetize Your AI Skills (outside of your 9‚Äì5 job)
5 technical side-hustles you can start this weekend
